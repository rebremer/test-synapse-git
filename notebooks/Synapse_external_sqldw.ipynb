{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "par_storage = \"testsynpriv2stor\"\r\n",
        "par_container = \"test\"\r\n",
        "par_csv_file = 'testcsv.txt'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 53,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:41:46.7964497Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:41:46.8895014Z",
              "execution_finish_time": "2021-11-09T08:41:46.889596Z"
            },
            "text/plain": "StatementMeta(test, 2, 53, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 41,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import socket\r\n",
        "\r\n",
        "addr = socket.gethostbyname(par_storage + '.dfs.core.windows.net')\r\n",
        "\r\n",
        "print(addr)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 54,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:41:46.8545146Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:41:46.9778378Z",
              "execution_finish_time": "2021-11-09T08:41:47.1284315Z"
            },
            "text/plain": "StatementMeta(test, 2, 54, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.250.0.10"
          ]
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "# Proof that synapse workspace can load data from attached storage account using private IP\r\n",
        "df = spark.read.load('abfss://' + par_container + '@' + par_storage + '.dfs.core.windows.net/' + par_csv_file, format='text')\r\n",
        "display(df.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 55,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:41:46.9758332Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:41:47.2207781Z",
              "execution_finish_time": "2021-11-09T08:41:47.7208406Z"
            },
            "text/plain": "StatementMeta(test, 2, 55, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "d0638561-d193-4ae3-aa38-eb2f1b5feac2",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": "SynapseWidget(Synapse.DataFrame, d0638561-d193-4ae3-aa38-eb2f1b5feac2)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 43,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "val par_sqldw_server = \"test-edlprod1-dbs.database.windows.net\"\n",
        "val par_sqldw_db_schema_table = \"test-synapse-sqldw.dbo.AdultCensusIncome4\"\n",
        "\n",
        "// SQLDW outside Synapse analytics. In this example, SQLDW table data originates from csv in this folder: AdultCensusIncome_withheader.csv. Make sure that the Synapse Workspace MI is registered as external user in the external SQLDWimport com.microsoft.spark.sqlanalytics.utils.Constants\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\n",
        "\n",
        "val df = spark.read.\n",
        "option(Constants.SERVER, par_sqldw_server).\n",
        "sqlanalytics(par_sqldw_db_schema_table)\n",
        "df.createOrReplaceTempView(\"temp_table\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 56,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:41:47.1600886Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:41:47.8284297Z",
              "execution_finish_time": "2021-11-09T08:41:50.5846755Z"
            },
            "text/plain": "StatementMeta(test, 2, 56, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "par_sqldw_server: String = test-edlprod1-dbs.database.windows.net\npar_sqldw_db_schema_table: String = test-synapse-sqldw.dbo.AdultCensusIncome4\nimport org.apache.spark.sql.SqlAnalyticsConnector._\ndf: org.apache.spark.sql.DataFrame = [age: int, workclass: string ... 13 more fields]\n"
          ]
        }
      ],
      "execution_count": 44,
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# To be able to complete this lab in under an hour, let's just work with 1,500k rows\n",
        "data_all = spark.sql(\"SELECT * FROM temp_table\").limit(15000000)\n",
        "#sample.createOrReplaceTempView(\"AdultCensusIncome\")\n",
        "\n",
        "columns_new = [col.replace(\"-\", \"\") for col in data_all.columns]\n",
        "data_all = data_all.toDF(*columns_new)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 57,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:41:47.2467946Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:41:50.67638Z",
              "execution_finish_time": "2021-11-09T08:41:50.8209149Z"
            },
            "text/plain": "StatementMeta(test, 2, 57, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 45,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data_all.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 58,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:41:47.3822229Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:41:50.9098588Z",
              "execution_finish_time": "2021-11-09T08:41:51.0520676Z"
            },
            "text/plain": "StatementMeta(test, 2, 58, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- age: integer (nullable = false)\n |-- workclass: string (nullable = false)\n |-- fnlwgt: integer (nullable = false)\n |-- education: string (nullable = false)\n |-- educationnum: integer (nullable = false)\n |-- maritalstatus: string (nullable = false)\n |-- occupation: string (nullable = false)\n |-- relationship: string (nullable = false)\n |-- race: string (nullable = false)\n |-- sex: string (nullable = false)\n |-- capitalgain: integer (nullable = false)\n |-- capitalloss: integer (nullable = false)\n |-- hoursperweek: integer (nullable = false)\n |-- nativecountry: string (nullable = false)\n |-- income: string (nullable = false)"
          ]
        }
      ],
      "execution_count": 46,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# In case public access is disabled from the storage account attached to Synapse, Spark pool can't create the temp table and stack trace below is created\r\n",
        "# In case public access is enabled in the storage data, temp table can be created on storage account and data can be loaded correctly\r\n",
        "#\r\n",
        "# It seems that org.apache.spark.sql.SqlAnalyticsConnector._ tries to acces to storage account using the public DNS rather than the private DNS\r\n",
        "\r\n",
        "display(data_all.limit(10))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 59,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:41:47.4638095Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:41:51.1419765Z",
              "execution_finish_time": "2021-11-09T08:41:54.9838637Z"
            },
            "text/plain": "StatementMeta(test, 2, 59, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:com.microsoft.spark.notebook.visualization.display.executeWithReturnVal.\n: com.microsoft.spark.sqlanalytics.exception.SQLAnalyticsConnectorException: com.microsoft.sqlserver.jdbc.SQLServerException: CREATE EXTERNAL TABLE AS SELECT statement failed as the path name 'abfss://synapse@testsynpriv2stor.dfs.core.windows.net/synapse/workspaces/test-synpriv2-syn/sparkpools/test/sparkpoolinstances/e1594f0f-768c-45cb-b84d-7719fb0ec499/livysessions/2021/11/09/2/tempdata/SQLAnalyticsConnectorStaging/application_1636443688496_0002/aUUYkzHI5xA6181fe1f98cd4afb931814c9d6c6f84d.tbl' could not be used for export. Please ensure that the specified path is a directory which exists or can be created, and that files can be created in that directory.\n\tat com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader$PlanInputPartitionsUtilities$.extractDataAndGetLocation(SQLAnalyticsReader.scala:256)\n\tat com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader.planBatchInputPartitions(SQLAnalyticsReader.scala:119)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.batchPartitions$lzycompute(DataSourceV2ScanExec.scala:84)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.batchPartitions(DataSourceV2ScanExec.scala:80)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.outputPartitioning(DataSourceV2ScanExec.scala:60)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$1.apply(EnsureRequirements.scala:150)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$1.apply(EnsureRequirements.scala:149)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(EnsureRequirements.scala:149)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:326)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:318)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:291)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:318)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:38)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:120)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:102)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measureTime(QueryPlanningTracker.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:101)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:109)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:107)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3051)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3049)\n\tat com.microsoft.spark.notebook.visualization.display$.exec(Display.scala:167)\n\tat com.microsoft.spark.notebook.visualization.display$.executeWithReturnVal(Display.scala:122)\n\tat com.microsoft.spark.notebook.visualization.display.executeWithReturnVal(Display.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
          "traceback": [
            "Py4JJavaError: An error occurred while calling z:com.microsoft.spark.notebook.visualization.display.executeWithReturnVal.\n: com.microsoft.spark.sqlanalytics.exception.SQLAnalyticsConnectorException: com.microsoft.sqlserver.jdbc.SQLServerException: CREATE EXTERNAL TABLE AS SELECT statement failed as the path name 'abfss://synapse@testsynpriv2stor.dfs.core.windows.net/synapse/workspaces/test-synpriv2-syn/sparkpools/test/sparkpoolinstances/e1594f0f-768c-45cb-b84d-7719fb0ec499/livysessions/2021/11/09/2/tempdata/SQLAnalyticsConnectorStaging/application_1636443688496_0002/aUUYkzHI5xA6181fe1f98cd4afb931814c9d6c6f84d.tbl' could not be used for export. Please ensure that the specified path is a directory which exists or can be created, and that files can be created in that directory.\n\tat com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader$PlanInputPartitionsUtilities$.extractDataAndGetLocation(SQLAnalyticsReader.scala:256)\n\tat com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader.planBatchInputPartitions(SQLAnalyticsReader.scala:119)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.batchPartitions$lzycompute(DataSourceV2ScanExec.scala:84)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.batchPartitions(DataSourceV2ScanExec.scala:80)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.outputPartitioning(DataSourceV2ScanExec.scala:60)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$1.apply(EnsureRequirements.scala:150)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$1.apply(EnsureRequirements.scala:149)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(EnsureRequirements.scala:149)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:326)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:318)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:291)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:318)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:38)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:120)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:102)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measureTime(QueryPlanningTracker.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:101)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:109)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:107)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3051)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3049)\n\tat com.microsoft.spark.notebook.visualization.display$.exec(Display.scala:167)\n\tat com.microsoft.spark.notebook.visualization.display$.executeWithReturnVal(Display.scala:122)\n\tat com.microsoft.spark.notebook.visualization.display.executeWithReturnVal(Display.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/trusted-service-user/cluster-env/env/lib/python3.6/site-packages/notebookutils/visualization/display.py\", line 77, in _execute\n    print(sc._jvm.display.executeWithReturnVal(commId))\n",
            "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n    return f(*a, **kw)\n",
            "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling z:com.microsoft.spark.notebook.visualization.display.executeWithReturnVal.\n: com.microsoft.spark.sqlanalytics.exception.SQLAnalyticsConnectorException: com.microsoft.sqlserver.jdbc.SQLServerException: CREATE EXTERNAL TABLE AS SELECT statement failed as the path name 'abfss://synapse@testsynpriv2stor.dfs.core.windows.net/synapse/workspaces/test-synpriv2-syn/sparkpools/test/sparkpoolinstances/e1594f0f-768c-45cb-b84d-7719fb0ec499/livysessions/2021/11/09/2/tempdata/SQLAnalyticsConnectorStaging/application_1636443688496_0002/aUUYkzHI5xA6181fe1f98cd4afb931814c9d6c6f84d.tbl' could not be used for export. Please ensure that the specified path is a directory which exists or can be created, and that files can be created in that directory.\n\tat com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader$PlanInputPartitionsUtilities$.extractDataAndGetLocation(SQLAnalyticsReader.scala:256)\n\tat com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader.planBatchInputPartitions(SQLAnalyticsReader.scala:119)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.batchPartitions$lzycompute(DataSourceV2ScanExec.scala:84)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.batchPartitions(DataSourceV2ScanExec.scala:80)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.outputPartitioning(DataSourceV2ScanExec.scala:60)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$1.apply(EnsureRequirements.scala:150)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$1.apply(EnsureRequirements.scala:149)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(EnsureRequirements.scala:149)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:326)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:318)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:291)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:318)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:38)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:120)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:102)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measureTime(QueryPlanningTracker.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:101)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:109)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:107)\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3051)\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3049)\n\tat com.microsoft.spark.notebook.visualization.display$.exec(Display.scala:167)\n\tat com.microsoft.spark.notebook.visualization.display$.executeWithReturnVal(Display.scala:122)\n\tat com.microsoft.spark.notebook.visualization.display.executeWithReturnVal(Display.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"
          ]
        }
      ],
      "execution_count": 47,
      "metadata": {
        "collapsed": false,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "workclass"
            ],
            "values": [
              "age"
            ],
            "yLabel": "age",
            "xLabel": "workclass",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"age\":{\" ?\":25,\" Private\":233,\" Self-emp-not-inc\":67,\" State-gov\":35}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {
        "d0638561-d193-4ae3-aa38-eb2f1b5feac2": {
          "type": "Synapse.DataFrame",
          "sync_state": {
            "table": {
              "rows": [
                {
                  "0": "firstname, lastname"
                },
                {
                  "0": "Rene, Bremer"
                },
                {
                  "0": "Thom, Bremer"
                },
                {
                  "0": "Stef, Bremer"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "value",
                  "type": "string"
                }
              ]
            },
            "isSummary": false,
            "language": "scala"
          },
          "persist_state": {
            "view": {
              "type": "details",
              "tableOptions": {},
              "chartOptions": {
                "chartType": "bar",
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "seriesFieldKeys": [
                  "0"
                ],
                "isStacked": false
              }
            }
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}