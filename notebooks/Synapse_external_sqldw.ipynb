{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "par_storage = \"testsynpriv2stor\" # default storage account attached to Synapse, private link connection between Synapse and storage account created\r\n",
        "par_container = \"test\"\r\n",
        "par_csv_file = 'testcsv.txt'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 93,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:53:22.762695Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:53:22.8581039Z",
              "execution_finish_time": "2021-11-09T08:53:22.9992115Z"
            },
            "text/plain": "StatementMeta(test, 2, 93, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 81,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import socket\r\n",
        "\r\n",
        "addr = socket.gethostbyname(par_storage + '.dfs.core.windows.net')\r\n",
        "\r\n",
        "print(addr)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 94,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:53:22.8335144Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:53:23.0990796Z",
              "execution_finish_time": "2021-11-09T08:53:23.2467378Z"
            },
            "text/plain": "StatementMeta(test, 2, 94, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.250.0.10"
          ]
        }
      ],
      "execution_count": 82,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "# Proof that synapse workspace can load data from attached storage account using private IP\r\n",
        "df = spark.read.load('abfss://' + par_container + '@' + par_storage + '.dfs.core.windows.net/' + par_csv_file, format='text')\r\n",
        "df.limit(10).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 95,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:53:22.964543Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:53:23.3420736Z",
              "execution_finish_time": "2021-11-09T08:53:24.3934812Z"
            },
            "text/plain": "StatementMeta(test, 2, 95, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n|              value|\n+-------------------+\n|firstname, lastname|\n|       Rene, Bremer|\n|       Thom, Bremer|\n|       Stef, Bremer|\n+-------------------+"
          ]
        }
      ],
      "execution_count": 83,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "val par_sqldw_server = \"test-edlprod1-dbs.database.windows.net\"\n",
        "val par_sqldw_db_schema_table = \"test-synapse-sqldw.dbo.AdultCensusIncome4\"\n",
        "\n",
        "// SQLDW outside Synapse analytics. In this example, SQLDW table data originates from csv in this folder: AdultCensusIncome_withheader.csv. Make sure that the Synapse Workspace MI is registered as external user in the external SQLDWimport com.microsoft.spark.sqlanalytics.utils.Constants\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\n",
        "\n",
        "val df = spark.read.\n",
        "option(Constants.SERVER, par_sqldw_server).\n",
        "sqlanalytics(par_sqldw_db_schema_table)\n",
        "df.createOrReplaceTempView(\"temp_table\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 96,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:53:23.0520655Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:53:24.4872414Z",
              "execution_finish_time": "2021-11-09T08:53:27.2027387Z"
            },
            "text/plain": "StatementMeta(test, 2, 96, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "par_sqldw_server: String = test-edlprod1-dbs.database.windows.net\npar_sqldw_db_schema_table: String = test-synapse-sqldw.dbo.AdultCensusIncome4\nimport org.apache.spark.sql.SqlAnalyticsConnector._\ndf: org.apache.spark.sql.DataFrame = [age: int, workclass: string ... 13 more fields]\n"
          ]
        }
      ],
      "execution_count": 84,
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# To be able to complete this lab in under an hour, let's just work with 1,500k rows\n",
        "data_all = spark.sql(\"SELECT * FROM temp_table\").limit(15000000)\n",
        "#sample.createOrReplaceTempView(\"AdultCensusIncome\")\n",
        "\n",
        "columns_new = [col.replace(\"-\", \"\") for col in data_all.columns]\n",
        "data_all = data_all.toDF(*columns_new)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 97,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:53:23.2020549Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:53:27.2905258Z",
              "execution_finish_time": "2021-11-09T08:53:27.4400829Z"
            },
            "text/plain": "StatementMeta(test, 2, 97, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 85,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data_all.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 98,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:53:23.3188284Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:53:27.5329922Z",
              "execution_finish_time": "2021-11-09T08:53:27.685366Z"
            },
            "text/plain": "StatementMeta(test, 2, 98, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- age: integer (nullable = false)\n |-- workclass: string (nullable = false)\n |-- fnlwgt: integer (nullable = false)\n |-- education: string (nullable = false)\n |-- educationnum: integer (nullable = false)\n |-- maritalstatus: string (nullable = false)\n |-- occupation: string (nullable = false)\n |-- relationship: string (nullable = false)\n |-- race: string (nullable = false)\n |-- sex: string (nullable = false)\n |-- capitalgain: integer (nullable = false)\n |-- capitalloss: integer (nullable = false)\n |-- hoursperweek: integer (nullable = false)\n |-- nativecountry: string (nullable = false)\n |-- income: string (nullable = false)"
          ]
        }
      ],
      "execution_count": 86,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# In case public access is disabled from the storage account attached to Synapse (par_storage), Spark pool can't create the temp table and stack trace below is created\r\n",
        "# In case public access is enabled in the storage account attached to Synapse (par_storage), temp table can be created on storage account and data can be loaded correctly\r\n",
        "#\r\n",
        "# It seems that org.apache.spark.sql.SqlAnalyticsConnector._ tries to acces to storage account using the public DNS rather than the private DNS\r\n",
        "\r\n",
        "data_all.limit(10).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 99,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:53:23.3993354Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:53:27.7741351Z",
              "execution_finish_time": "2021-11-09T08:53:31.6346569Z"
            },
            "text/plain": "StatementMeta(test, 2, 99, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o1032.showString.\n: com.microsoft.spark.sqlanalytics.exception.SQLAnalyticsConnectorException: com.microsoft.sqlserver.jdbc.SQLServerException: CREATE EXTERNAL TABLE AS SELECT statement failed as the path name 'abfss://synapse@testsynpriv2stor.dfs.core.windows.net/synapse/workspaces/test-synpriv2-syn/sparkpools/test/sparkpoolinstances/e1594f0f-768c-45cb-b84d-7719fb0ec499/livysessions/2021/11/09/2/tempdata/SQLAnalyticsConnectorStaging/application_1636443688496_0002/aVESnt9Jcgz6181fe1f98cd4afb931814c9d6c6f84d.tbl' could not be used for export. Please ensure that the specified path is a directory which exists or can be created, and that files can be created in that directory.\n\tat com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader$PlanInputPartitionsUtilities$.extractDataAndGetLocation(SQLAnalyticsReader.scala:256)\n\tat com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader.planBatchInputPartitions(SQLAnalyticsReader.scala:119)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.batchPartitions$lzycompute(DataSourceV2ScanExec.scala:84)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.batchPartitions(DataSourceV2ScanExec.scala:80)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.outputPartitioning(DataSourceV2ScanExec.scala:60)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$1.apply(EnsureRequirements.scala:150)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$1.apply(EnsureRequirements.scala:149)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(EnsureRequirements.scala:149)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:326)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:318)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:291)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:318)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:38)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:120)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:102)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measureTime(QueryPlanningTracker.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:101)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:94)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3373)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2558)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2772)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:262)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:299)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
          "traceback": [
            "Py4JJavaError: An error occurred while calling o1032.showString.\n: com.microsoft.spark.sqlanalytics.exception.SQLAnalyticsConnectorException: com.microsoft.sqlserver.jdbc.SQLServerException: CREATE EXTERNAL TABLE AS SELECT statement failed as the path name 'abfss://synapse@testsynpriv2stor.dfs.core.windows.net/synapse/workspaces/test-synpriv2-syn/sparkpools/test/sparkpoolinstances/e1594f0f-768c-45cb-b84d-7719fb0ec499/livysessions/2021/11/09/2/tempdata/SQLAnalyticsConnectorStaging/application_1636443688496_0002/aVESnt9Jcgz6181fe1f98cd4afb931814c9d6c6f84d.tbl' could not be used for export. Please ensure that the specified path is a directory which exists or can be created, and that files can be created in that directory.\n\tat com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader$PlanInputPartitionsUtilities$.extractDataAndGetLocation(SQLAnalyticsReader.scala:256)\n\tat com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader.planBatchInputPartitions(SQLAnalyticsReader.scala:119)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.batchPartitions$lzycompute(DataSourceV2ScanExec.scala:84)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.batchPartitions(DataSourceV2ScanExec.scala:80)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.outputPartitioning(DataSourceV2ScanExec.scala:60)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$1.apply(EnsureRequirements.scala:150)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$1.apply(EnsureRequirements.scala:149)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(EnsureRequirements.scala:149)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:326)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:318)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:291)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:318)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:38)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:120)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:102)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measureTime(QueryPlanningTracker.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:101)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:94)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3373)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2558)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2772)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:262)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:299)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 380, in show\n    print(self._jdf.showString(n, 20, vertical))\n",
            "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n    return f(*a, **kw)\n",
            "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o1032.showString.\n: com.microsoft.spark.sqlanalytics.exception.SQLAnalyticsConnectorException: com.microsoft.sqlserver.jdbc.SQLServerException: CREATE EXTERNAL TABLE AS SELECT statement failed as the path name 'abfss://synapse@testsynpriv2stor.dfs.core.windows.net/synapse/workspaces/test-synpriv2-syn/sparkpools/test/sparkpoolinstances/e1594f0f-768c-45cb-b84d-7719fb0ec499/livysessions/2021/11/09/2/tempdata/SQLAnalyticsConnectorStaging/application_1636443688496_0002/aVESnt9Jcgz6181fe1f98cd4afb931814c9d6c6f84d.tbl' could not be used for export. Please ensure that the specified path is a directory which exists or can be created, and that files can be created in that directory.\n\tat com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader$PlanInputPartitionsUtilities$.extractDataAndGetLocation(SQLAnalyticsReader.scala:256)\n\tat com.microsoft.spark.sqlanalytics.read.SQLAnalyticsReader.planBatchInputPartitions(SQLAnalyticsReader.scala:119)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.batchPartitions$lzycompute(DataSourceV2ScanExec.scala:84)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.batchPartitions(DataSourceV2ScanExec.scala:80)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExec.outputPartitioning(DataSourceV2ScanExec.scala:60)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$1.apply(EnsureRequirements.scala:150)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$1.apply(EnsureRequirements.scala:149)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(EnsureRequirements.scala:149)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:326)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$apply$1.applyOrElse(EnsureRequirements.scala:318)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:292)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:291)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:343)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:196)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:341)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:289)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:318)\n\tat org.apache.spark.sql.execution.exchange.EnsureRequirements.apply(EnsureRequirements.scala:38)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$prepareForExecution$1.apply(QueryExecution.scala:120)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution.prepareForExecution(QueryExecution.scala:120)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:102)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$executedPlan$1.apply(QueryExecution.scala:102)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measureTime(QueryPlanningTracker.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:101)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:94)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3373)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2558)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2772)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:262)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:299)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"
          ]
        }
      ],
      "execution_count": 87,
      "metadata": {
        "collapsed": false,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "workclass"
            ],
            "values": [
              "age"
            ],
            "yLabel": "age",
            "xLabel": "workclass",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"age\":{\" ?\":25,\" Private\":233,\" Self-emp-not-inc\":67,\" State-gov\":35}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}