{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "par_storage = \"testsynpriv2stor\" # default storage account attached to Synapse, private link connection between Synapse and storage account created\r\n",
        "par_container = \"test\"\r\n",
        "par_csv_file = 'testcsv.txt'"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 86,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:51:50.3363021Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:51:50.4367746Z",
              "execution_finish_time": "2021-11-09T08:51:50.5935249Z"
            },
            "text/plain": "StatementMeta(test, 2, 86, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 74,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import socket\r\n",
        "\r\n",
        "addr = socket.gethostbyname(par_storage + '.dfs.core.windows.net')\r\n",
        "\r\n",
        "print(addr)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 87,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:51:50.4496055Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:51:50.6847917Z",
              "execution_finish_time": "2021-11-09T08:51:50.8268807Z"
            },
            "text/plain": "StatementMeta(test, 2, 87, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.250.0.10"
          ]
        }
      ],
      "execution_count": 75,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "# Proof that synapse workspace can load data from attached storage account using private IP\r\n",
        "df = spark.read.load('abfss://' + par_container + '@' + par_storage + '.dfs.core.windows.net/' + par_csv_file, format='text')\r\n",
        "df.limit(10).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 88,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:51:50.5509113Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:51:50.9149078Z",
              "execution_finish_time": "2021-11-09T08:51:51.414754Z"
            },
            "text/plain": "StatementMeta(test, 2, 88, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n|              value|\n+-------------------+\n|firstname, lastname|\n|       Rene, Bremer|\n|       Thom, Bremer|\n|       Stef, Bremer|\n+-------------------+"
          ]
        }
      ],
      "execution_count": 76,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "val par_sqldw_server = \"test-edlprod1-dbs.database.windows.net\"\n",
        "val par_sqldw_db_schema_table = \"test-synapse-sqldw.dbo.AdultCensusIncome4\"\n",
        "\n",
        "// SQLDW outside Synapse analytics. In this example, SQLDW table data originates from csv in this folder: AdultCensusIncome_withheader.csv. Make sure that the Synapse Workspace MI is registered as external user in the external SQLDWimport com.microsoft.spark.sqlanalytics.utils.Constants\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\n",
        "\n",
        "val df = spark.read.\n",
        "option(Constants.SERVER, par_sqldw_server).\n",
        "sqlanalytics(par_sqldw_db_schema_table)\n",
        "df.createOrReplaceTempView(\"temp_table\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 89,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:51:50.6381023Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:51:51.5015506Z",
              "execution_finish_time": "2021-11-09T08:51:54.2844437Z"
            },
            "text/plain": "StatementMeta(test, 2, 89, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "par_sqldw_server: String = test-edlprod1-dbs.database.windows.net\npar_sqldw_db_schema_table: String = test-synapse-sqldw.dbo.AdultCensusIncome4\nimport org.apache.spark.sql.SqlAnalyticsConnector._\ndf: org.apache.spark.sql.DataFrame = [age: int, workclass: string ... 13 more fields]\n"
          ]
        }
      ],
      "execution_count": 77,
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# To be able to complete this lab in under an hour, let's just work with 1,500k rows\n",
        "data_all = spark.sql(\"SELECT * FROM temp_table\").limit(15000000)\n",
        "#sample.createOrReplaceTempView(\"AdultCensusIncome\")\n",
        "\n",
        "columns_new = [col.replace(\"-\", \"\") for col in data_all.columns]\n",
        "data_all = data_all.toDF(*columns_new)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 90,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:51:50.7489687Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:51:54.3798678Z",
              "execution_finish_time": "2021-11-09T08:51:54.5191963Z"
            },
            "text/plain": "StatementMeta(test, 2, 90, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 78,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data_all.printSchema()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 91,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-11-09T08:51:50.908973Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:51:54.6090867Z",
              "execution_finish_time": "2021-11-09T08:51:54.7526467Z"
            },
            "text/plain": "StatementMeta(test, 2, 91, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n |-- age: integer (nullable = false)\n |-- workclass: string (nullable = false)\n |-- fnlwgt: integer (nullable = false)\n |-- education: string (nullable = false)\n |-- educationnum: integer (nullable = false)\n |-- maritalstatus: string (nullable = false)\n |-- occupation: string (nullable = false)\n |-- relationship: string (nullable = false)\n |-- race: string (nullable = false)\n |-- sex: string (nullable = false)\n |-- capitalgain: integer (nullable = false)\n |-- capitalloss: integer (nullable = false)\n |-- hoursperweek: integer (nullable = false)\n |-- nativecountry: string (nullable = false)\n |-- income: string (nullable = false)"
          ]
        }
      ],
      "execution_count": 79,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# In case public access is disabled from the storage account attached to Synapse (par_storage), Spark pool can't create the temp table and stack trace below is created\r\n",
        "# In case public access is enabled in the storage account attached to Synapse (par_storage), temp table can be created on storage account and data can be loaded correctly\r\n",
        "#\r\n",
        "# It seems that org.apache.spark.sql.SqlAnalyticsConnector._ tries to acces to storage account using the public DNS rather than the private DNS\r\n",
        "\r\n",
        "data_all.limit(10).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "test",
              "session_id": 2,
              "statement_id": 92,
              "state": "submitted",
              "livy_statement_state": "running",
              "queued_time": "2021-11-09T08:51:51.0483706Z",
              "session_start_time": null,
              "execution_start_time": "2021-11-09T08:51:54.8460533Z",
              "execution_finish_time": null
            },
            "text/plain": "StatementMeta(test, 2, 92, Submitted, Running)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 80,
      "metadata": {
        "collapsed": false,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "workclass"
            ],
            "values": [
              "age"
            ],
            "yLabel": "age",
            "xLabel": "workclass",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"age\":{\" ?\":25,\" Private\":233,\" Self-emp-not-inc\":67,\" State-gov\":35}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}